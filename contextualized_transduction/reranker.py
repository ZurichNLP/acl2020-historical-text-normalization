from typing import List, Union, Generator, Tuple, Callable, Optional, Sequence, Any, Dict
import os
from pathlib import Path
from collections import namedtuple, Counter
import json
import kenlm
import editdistance
import numpy as np
import abc
import heapq
import math
import time
import multiprocessing
from sklearn.linear_model import base, SGDClassifier

from contextualized_transduction.utils import scheduler_dequeue, scheduler_queue

from nn_lm.custom_lm import CharLanguageModel, WordLanguageModel

Sample: Tuple[str, str, str] = namedtuple('Sample', 'source target feats')
Prediction: Tuple[str, str, str, str, float, str] = namedtuple('Prediction',
                                                               'source feats hypothesis target score actions')
Predictions: Tuple[str, str, Sequence[str], str, Sequence[str], Sequence[Tuple[float]], Sequence[str]] = \
    namedtuple('Predictions', 'source feats hypotheses target scores actions')
TrainSample: Tuple[np.ndarray, float] = namedtuple('TrainSample', 'feat target')


class SimpleLanguageModel:
    def __init__(self, path2model: [Path, str], word_separator: str = 'รท'):
        self.path2model = str(path2model)
        self.model = kenlm.Model(self.path2model)
        self.word_separator = word_separator

    def score(self, words: Union[str, List[str]], rebase: bool = True) -> float:
        if isinstance(words, list):
            words = ' '.join(words)
        rebasing = np.log10(np.e) if rebase else 1.
        return self.model.score(words.replace(self.word_separator, ' '), bos=False, eos=False) / rebasing

    def score_batch(self, batch_of_words: List) -> List[float]:
        return [self.score(w) for w in batch_of_words]

    def __contains__(self, words: Union[str, List[str]]) -> bool:
        if isinstance(words, list):
            return all(word in self.model for word in words)
        else:
            return words in self.model


def direct_fn_reader(path2data: Union[str, Path], delimiter: str = '\t', encoding: str = 'utf8') -> \
        Generator[Sample, None, None]:
    """
    Read direct (source => target) delimiter-separated datasets.
    :param path2data: Path to dataset file.
    :param delimiter: Delimiter.
    :param encoding: Encoding.
    """
    with Path(path2data).open(encoding=encoding) as f:
        for line in f:
            line = line.rstrip()
            if line:
                try:
                    source, target, feats, *_rest = line.split(delimiter)
                except ValueError:
                    source, target = line.split(delimiter)
                    feats = None
                yield Sample(source, target, feats)


def dec_json_reader(path2data: Union[str, Path], encoding: str = 'utf8') -> Generator[Predictions, None, None]:
    """
    Read json files generated by `decoders.py` using sampling or beam search decoding.
    :param path2data: Path to json.
    :param encoding: Encoding.
    """
    def build_predictions(feats: Union[str, Sequence[str]], pred_value: Dict):

        # sort `candidates` and `log_prob` by candidates' length for char RNN LM mini-batch decoding
        candidates = pred_value['candidates']
        log_prob = pred_value['log_prob']
        actions = pred_value['acts']

        sorting_indices = np.argsort([len(p) for p in candidates])[::-1]

        log_prob = [log_prob[idx] for idx in sorting_indices]
        candidates = [candidates[idx] for idx in sorting_indices]
        actions = [actions[idx] for idx in sorting_indices]

        scores = [log_prob]
        # add any additional scores from this json
        if 'lm_scores' in pred_value:
            lm_scores = [pred_value['lm_scores'][idx] for idx in sorting_indices]
            scores.append(lm_scores)
        if 'channel_scores' in pred_value:
            channel_scores = [pred_value['channel_scores'][idx] for idx in sorting_indices]
            scores.append(channel_scores)
        scores = list(zip(*scores))

        target = pred_value['target']
        if isinstance(target, str):
            assert isinstance(feats, str), (source, target, feats)
            predictions = [Predictions(source, feats, candidates, target, scores, actions)]
        else:
            predictions = [Predictions(source, f, candidates, t, scores, actions)
                           for t, f in zip(target, feats)]
        return predictions

    with Path(path2data).open(encoding=encoding) as f:
        for source, feats_value in json.load(f).items():
            # lame way to detect the type of json input: repackaged or not
            if 'candidates' in feats_value and 'log_prob' in feats_value:
                yield from build_predictions(feats_value['feats'], feats_value)
            else:
                for feats, pred_value in feats_value.items():
                    yield from build_predictions(feats, pred_value)


class ScoreFunction(abc.ABC):
    """
    Function to assign real value to a hypothesis given target.
    """
    @abc.abstractmethod
    def score(self, target: Any, hypothesis: Any) -> float:
        """
        Compute the score for the hypothesis given the target.
        :param target: Target.
        :param hypothesis: Hypotesis.
        :return: Score.
        """
        pass


class NormalizedEditDistanceMargin(ScoreFunction):
    def __init__(self, margin: float = 1.):
        self.margin = margin

    def score(self, target: str, hypothesis: str) -> float:
        """
        The scores lie in [-self.margin, self.margin]. Semantics: completely off, completely correct.
        :param target: Target.
        :param hypothesis: Hypothesis.
        :return:
        """
        if target == hypothesis:
            return self.margin
        else:
            return - self.margin * editdistance.eval(target, hypothesis) / max(len(target), len(hypothesis))


class Featurizer(abc.ABC):
    """
    Convert predictions into input feature vectors (X), scores (y), and truth labels.
    """
    @abc.abstractmethod
    def _featurize(self, predictions: Predictions) -> List[np.ndarray]:
        """
        Convert a batch of predictions into a list of feature vector.
        :param prediction: Predictions.
        :return: Feature vectors.
        """
        pass

    def featurize(self, data: List[Predictions], score_function: ScoreFunction, *args, **kwargs) -> \
            Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
         Convert predictions into input feature vectors (X), scores (y), and true labels (does hypothesis match
            target?).
        :param data: List of `Predictions`.
        :param score_function: Score function for the computation of the scores of hypotheses given their targets
            from `Predictions`.
        :return: Feature vector array, scores array, true label array.
        """
        prediction_per_input, _ = Counter(len(p.hypotheses) for p in data).most_common(1)[0]
        # this assumes predictions for one input are following one another
        X: List[np.ndarray] = []
        Y: List[np.ndarray] = []
        Truth: List[np.ndarray] = []
        len_data = len(data)
        for prediction_num, predictions in enumerate(data):
            source, feats, hypotheses, target, *_ = predictions
            tmp_xs: List[np.ndarray] = self._featurize(predictions)
            tmp_ys: List[float] = []
            tmp_truth: List[float] = []
            for h, hypothesis in enumerate(hypotheses):
                score = score_function.score(hypothesis, target)
                tmp_ys.append(score)
                tmp_truth.append(hypothesis == target)
            if len(tmp_xs) != prediction_per_input:
                print(f'Input {(source, feats)} has an unexpected number of predictions: {len(tmp_xs)} vs '
                      f'{prediction_per_input}')
                if len(tmp_xs) < prediction_per_input:
                    diff = prediction_per_input - len(tmp_xs)
                    count = 0
                    while count < diff:
                        tmp_xs.append(tmp_xs[count])
                        tmp_ys.append(tmp_ys[count])
                        tmp_truth.append(tmp_truth[count])
                        count += 1
                else:
                    tmp_xs = tmp_xs[:prediction_per_input]
                    tmp_ys = tmp_ys[:prediction_per_input]
                    tmp_truth = tmp_truth[:prediction_per_input]
            X.append(np.array(tmp_xs))
            Y.append(np.array(tmp_ys))
            Truth.append(np.array(tmp_truth, dtype=np.bool))
            if prediction_num > 0 and prediction_num % 100 == 0:
                print(f'\tfeaturized {prediction_num}/{len_data} inputs...')
        X_ = np.array(X)
        Y_ = np.array(Y)
        Truth_ = np.array(Truth)

        # 1st dim: Inputs, 2nd dim: predictions, 3rd dim: features / score
        assert X_.ndim == 3 and Y_.ndim == 2, (X_.shape, Y_.shape)
        assert X_.shape[0] == Y_.shape[0], (X_.shape, Y_.shape)
        assert X_.shape[1] == Y_.shape[1] == prediction_per_input, (X_.shape, Y_.shape)
        assert Y_.shape == Truth_.shape, (Y_.shape, Truth_.shape)
        return X_, Y_, Truth_


def stepfunc(diff: float, step: float = 0.05) -> bool:
    """
    Simple step function to decide on accepting / rejecting a drawn sample.
    :param diff: The absolute difference between the scores of two samples hypotheses.
    :param step: Reject below this step value.
    :return: Accept or reject the sample based on `diff`.
    """
    assert diff >= 0, diff
    return diff >= step


class PROReranker:
    def __init__(self, tau: int = 500, alpha: Callable[[float], bool] = stepfunc,
                 cutoff: int = 100, eta: float = 0.1, epochs: int = 5, restarts: int = 1,
                 upper_bound_exact: int = 200, num_threads: int = 1,
                 random_seed: Optional[int] = None):
        """
        A PRO reranker (Hopkins & May EMNLP 2011) trained with perceptron.
        :param tau: Number of draws per input.
        :param alpha: Function to decide on accepting / rejecting a drawn sample.
        :param cutoff: Keep top-`cutoff` samples per input.
        :param eta: Perceptron learning rate.
        :param epochs: Epochs of reranker training (drawing a sample, then updating weights with perceptron).
        :param restarts: Number of restarts of reranker training.
        :param upper_bound_exact: Do not sample, but compute training data samples for all 2-combinations of
            predictions, if the number of predictions N is small:  N * (N-1) / 2 < `upper_bound_exact`
        :param num_threads: Number of threads to use in training (restarts are trivially parallelizable).
        :param random_seed: Seed to control all randomness of np.random.
        """
        self.tau = tau
        self.alpha = alpha
        self.cutoff = cutoff
        self.eta = eta
        self.epochs = epochs
        self.restarts = restarts
        self.upper_bound_exact = upper_bound_exact
        self.num_threads = num_threads
        self.random_seed = random_seed

        if self.random_seed is not None:
            np.random.seed(self.random_seed)

        self.fit = self._perceptron
        self.weights = None

    def _sample(self, X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Sample a binary-classification training dataset as described in the PRO paper. A binary label is the sign of
        the difference of the scores of two hypotheses (for the same input). The feature vector is the difference of
        their feature vectors.
        :param X: All hypotheses feature vectors.
        :param Y: All hypotheses scores.
        :return: Sampled binary dataset (feature vectors and labels).
        """
        num_inputs, num_predictions, num_feats = X.shape

        # total number of training items per input
        num_train_items = num_predictions * (num_predictions - 1)

        if num_train_items < self.upper_bound_exact:
            print('\tcomputing exactly...')
            # compute exactly, don't sample
            X_train = np.zeros((num_inputs * num_train_items, num_feats))
            y_train = np.zeros((num_inputs * num_train_items))
            next_item = 0
            for h in range(num_inputs):
                Xh = X[h]
                Yh = Y[h]
                for one in range(num_predictions):
                    for two in range(num_predictions):
                        if one < two:
                            diff = float(Yh[one] - Yh[two])
                            assert isinstance(diff, float), (diff, Yh)
                            abs_diff = abs(diff)
                            if self.alpha(abs_diff):
                                feat_vec = Xh[one] - Xh[two]
                                sign = np.sign(diff)
                                X_train[next_item] = feat_vec
                                y_train[next_item] = sign
                                next_item += 1
                                X_train[next_item] = -feat_vec
                                y_train[next_item] = -sign
                                next_item += 1
        else:
            print('\tsampling...')
            X_train = np.zeros((num_inputs * self.cutoff, num_feats))
            y_train = np.zeros((num_inputs * self.cutoff))
            count = 0  # to allow for minheap unique comparisons
            for h in range(num_inputs):
                sample = []  # minheap for each input
                Xh = X[h]
                Yh = Y[h]
                for _ in range(self.tau):
                    one, two = np.random.choice(num_predictions, size=2, replace=False)
                    diff = float(Yh[one] - Yh[two])
                    assert isinstance(diff, float), (diff, Yh)
                    abs_diff = abs(diff)
                    if self.alpha(abs_diff):
                        feat_vec = Xh[one] - Xh[two]
                        sign = np.sign(diff)
                        heapq.heappush(sample, (-diff, count, (feat_vec, sign)))
                        count += 1
                        heapq.heappush(sample, (-diff, count, (-feat_vec, -sign)))
                        count += 1
                if not sample:
                    continue
                curr_cutoff = min(self.cutoff, len(sample))
                Xh_train, yh_train = zip(*[(x, y) for _, _, (x, y) in
                                           (heapq.heappop(sample) for _ in range(curr_cutoff))])
                h_slice = slice(h * curr_cutoff, (h + 1) * curr_cutoff)
                X_train[h_slice] = np.array(Xh_train)
                y_train[h_slice] = yh_train

        nonzero_idx = np.nonzero(y_train)

        return X_train[nonzero_idx], y_train[nonzero_idx]

    def _perceptron(self, X_train: np.ndarray, y_train: np.ndarray, weights: Optional[np.ndarray] = None) -> np.ndarray:
        """
        Perceptron update of the reranker weights on a sampled binary classification dataset.
        :param X_train: Feature vectors.
        :param y_train: Binary labels.
        :param weights: Weights to update. If None, `self.weights` are used.
        :return: Updated weights.
        """
        print('\tSampled training set size: ', X_train.shape)
        correct = 0
        total = 0

        # shuffle
        idx = np.random.permutation(X_train.shape[0])
        X_train = X_train[idx]
        y_train = y_train[idx]

        weights = self.weights if weights is None else weights

        for i, x in enumerate(X_train):
            if np.sign(np.dot(weights, x)) == y_train[i]:
                correct += 1
            total += 1
        print(f'\tInitial sampled set accuracy:\t{correct * 100 / total:.1f}')

        correct = 0
        total = 0
        num_corrections = 0
        for i, x in enumerate(X_train):
            if np.sign(np.dot(weights, x)) != y_train[i]:
                # misclassification
                weights += self.eta * y_train[i] * x
                num_corrections += 1
            else:
                correct += 1
            total += 1
        print(f'\tFinal sampled set accuracy:\t{correct * 100 / total:.1f}. Number of corrections:\t{num_corrections}')
        return weights


    def _batch_train(self, input):

        q, X, y, truth, restart_range = input  # @TODO move X, y, truth to __init__

        # each sub-process needs its own random seed
        np.random.seed(q if self.random_seed is None else (q + self.random_seed))

        best_weights = self.weights
        best_acc = -1

        for restart in restart_range:
            print(f'\n\tRestart {restart}...')

            weights: np.ndarray = np.random.rand(X.shape[2])

            print('\tInitial type-level prediction accuracy:')
            _, acc = self.predict(X, y, truth, weights, prefix='\t')
            if acc > best_acc:
                best_acc = acc
                best_weights = np.copy(weights)

            for epoch in range(self.epochs):
                print(f'\tEpoch {epoch}...')
                X_train, y_train = self._sample(X, y)
                weights = self.fit(X_train, y_train, weights)
                _, acc = self.predict(X, y, truth, weights, prefix='\t')
                if acc > best_acc:
                    best_acc = acc
                    best_weights = np.copy(weights)

        return best_weights, best_acc

    def train(self, X: np.ndarray, y: np.ndarray, truth: np.ndarray) -> None:
        """
        Train reranker on hypotheses, their scores, and true labels.
        :param X: Hypotheses' feature vectors.
        :param y: Scores.
        :param truth: True labels (whether a hypothesis matches its target).
        """
        best_weights = self.weights
        best_acc = -1

        if self.num_threads > 1:
            try:
                scheduler_queue(lockfile_name=os.environ.get("CNNC_MULTIPROCESSING_RUNNING",
                                                             "CNNC_MULTIPROCESSING_RUNNING"))

                step_size = math.ceil(self.restarts / self.num_threads)
                grouped_restarts = [(q, X, y, truth, range(i, i + step_size))
                                    for q, i in enumerate(range(0, self.restarts, step_size))]
                pool = multiprocessing.Pool()
                results = pool.map(self._batch_train, grouped_restarts)
                pool.terminate()
                pool.join()
            finally:
                scheduler_dequeue(lockfile_name=os.environ.get("CNNC_MULTIPROCESSING_RUNNING",
                                                               "CNNC_MULTIPROCESSING_RUNNING"))


            for weights, acc in results:
                if acc > best_acc:
                    best_acc = acc
                    best_weights = weights

        else:
            for restart in range(self.restarts):
                print(f'\n\tRestart {restart}...')

                self.weights: np.ndarray = np.random.rand(X.shape[2])

                print('\tInitial type-level prediction accuracy:')
                _, acc = self.predict(X, y, truth, prefix='\t')
                if acc > best_acc:
                    best_acc = acc
                    best_weights = np.copy(self.weights)

                for epoch in range(self.epochs):
                    print(f'\tEpoch {epoch}...')
                    X_train, y_train = self._sample(X, y)
                    self.fit(X_train, y_train)
                    _, acc = self.predict(X, y, truth, prefix='\t')
                    if acc > best_acc:
                        best_acc = acc
                        best_weights = np.copy(self.weights)

        self.weights = best_weights

    def score(self, X: np.ndarray, weights: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Return sorting order and scores of hypotheses.
        :param X: Hypotheses' feature vectors.
        :param weights: Weights to use. If None, `self.weights` are used.
        :return: An array of sorting indices and an array of scores.
        """
        # num_inputs, num_predictions, num_feats = X.shape
        scores = np.dot(X, self.weights if weights is None else weights)
        sorting_indices = scores.argsort(axis=1)[:, ::-1]  # descending order
        return sorting_indices, scores

    def predict(self, X: np.ndarray, y: Optional[np.ndarray] = None, truth: Optional[np.ndarray] = None,
                weights: Optional[np.ndarray] = None, prefix='') -> Tuple[np.ndarray, Optional[float]]:
        """
        Select best hypotheses from X and evaluate them against truth.
        :param X: Hypotheses' feature vectors.
        :param y: Scores.
        :param truth: True labels (whether a hypothesis matches its target).
        :param weights: Weights to use. If None, `self.weights` are used.
        :param prefix: Any prefix used for printing.
        :return: Array with indices of one best hypothesis per input.
        """

        scores = np.dot(X, self.weights if weights is None else weights)
        predictions = scores.argmax(axis=1)

        if truth is not None:
            acc = 100 * sum(ts[predictions[h]] for h, ts in enumerate(truth)) / truth.shape[0]
            print(f'{prefix}Type-level prediction accuracy:\t{acc:.1f}')
        else:
            acc = None

        return predictions, acc

    def load_from_file(self, path: Union[str, Path]):
        self.weights = np.load(path)

    def save_to_file(self, path: Union[str, Path]):
        np.save(path, self.weights)


class CoreFeaturizer(Featurizer):

    def __init__(self, language_model: Optional[SimpleLanguageModel],
                 train_data: List[Sample],
                 char_nn_language_model: Optional[CharLanguageModel] = None,
                 word_nn_language_model: Optional[WordLanguageModel] = None,
                 prefix_size: int = 2, suffix_size: int = 2, pseudocount: int = 1.,
                 word_unk_score: Optional[float] = None):
        """
        Tuns a prediction into a feature vector using the following features:
          * language model score (hypothesis)
          * direct model score (hypothesis | source)
          * levenshtein distance (hypothesis, source)
          * normalized levenshtein distance (hypothesis, source)
          * (source, hypothesis) seen in training?
          * relative frequency of (source, hypothesis) in training
          * same suffix (e.g. last 2) (hypothesis, source)
          * same prefix (e.g. first 2) (hypothesis, source)
          * source is a subsequence of hypothesis?
          * hypothesis is a subsequence of source?

        :param language_model: Language model.
        :param char_nn_language_model: Character-level RNN language model.
        :param word_nn_language_model: Word-level RNN language model.
        :param train_data: Training data to derive some statistics from (e.g. source, hypothesis relative frequencies).
        :param prefix_size: The length of prefix to compare between source and hypothesis.
        :param suffix_size: The length of suffix to compare between source and hypothesis.
        :param pseudocount: Pseudocount for computing UNK probability for decoding.
        :param word_unk_score: If not None, will use this ln score for <unk>. This could be beneficial as, if <unk> had
            a high relative frequency in the training data, it will get unreasonably high scores by the model.
        """
        self.language_model = language_model
        if self.language_model:
            self.lm_score = self.language_model.score_batch
        else:
            self.lm_score = lambda x: [0.]*len(x)
        self.char_nn_language_model = char_nn_language_model
        if self.char_nn_language_model:
            self.char_nn_lm_score = self.char_nn_language_model.score_batch
        else:
            self.char_nn_lm_score = lambda x: [0.]*len(x)
        self.word_nn_language_model = word_nn_language_model
        if self.word_nn_language_model:
            self.word_nn_lm_score = (lambda words:
                list(zip(*self.word_nn_language_model.score_word_batch(words, unk_score=self.word_unk_score)))[1])
            # outputs list of states, scores; take scores
        else:
            self.word_nn_lm_score = lambda x: [0.]*len(x)
        self.word_unk_score = word_unk_score

        self.train_data = train_data

        self.train_set_uniq = set(self.train_data)
        if self.train_data:
            num_types = len(self.train_set_uniq)
            len_train_data = len(self.train_data)
            self.discount = pseudocount / len_train_data
            self.train_counter = {sample: (count + pseudocount) / (len_train_data + pseudocount * num_types)
                                  for sample, count in Counter(self.train_data).items()}
        else:
            self.discount = 0.
            self.train_counter = dict()
        self.prefix_size = prefix_size
        self.suffix_size = suffix_size

    def _featurize(self, predictions: Predictions) -> List[np.ndarray]:
        """
        Turn a batch of hypotheses into feature vectors.
        :param predictions: Predictions.
        :return: Feature vectors.
        """
        feature_vectors: List[np.ndarray] = []

        source, feats, hypotheses, target, scores, actions = predictions
        lm_scores = self.lm_score(hypotheses)
        char_nn_scores = self.char_nn_lm_score(hypotheses)  # / np.array([len(s) for s in hypotheses])
        word_nn_scores = self.word_nn_lm_score(hypotheses)

        for i, (score, hypothesis) in enumerate(zip(scores, hypotheses)):
            sample = Sample(source, hypothesis, feats)
            ld = editdistance.eval(hypothesis, source)
            feature_vector = np.array([
                1.,
                lm_scores[i],
                char_nn_scores[i],
                word_nn_scores[i],
                *score,
                ld,
                ld / len(source),
                float(sample in self.train_set_uniq),
                self.train_counter.get(sample, self.discount),
                float(hypothesis[:self.prefix_size] == source[:self.prefix_size]),
                float(hypothesis[-self.suffix_size:] == source[-self.prefix_size:]),
                float(source in hypothesis),
                float(hypothesis in source)
            ])
            feature_vectors.append(feature_vector)
        return feature_vectors


class PROClassifierReranker(PROReranker):

    def __init__(self, classifier: base, *args, **kargs):
        super().__init__(*args, **kargs)
        self.classifier = classifier
        self.fit = self._fit

    def _fit(self, X_train: np.ndarray, y_train: np.ndarray) -> None:

        acc = (np.sign(np.dot(X_train, self.weights.T)) == y_train).sum() / y_train.size
        print(f'\tInitial sampled set accuracy:\t{100 * acc:.1f}')

        self.classifier.fit(X_train, y_train, coef_init=np.array([self.weights]), intercept_init=np.zeros(1))
        self.weights = self.classifier.coef_[0]  # (1, num_feats)

        acc = (np.sign(np.dot(X_train, self.weights.T)) == y_train).sum() / y_train.size
        print(f'\tFinal sampled set accuracy:\t{100 * acc:.1f}')


class WordReranker:

    def __init__(self, config):

        # paths
        model_dir = config.get('model_dir')
        reranker_dir = config.get('reranker_dir')
        dev_dec_json = config['dev_dec_json']
        test_dec_json = config.get('test_dec_json')
        test2_dec_json = config.get('test2_dec_json')
        train = config.get('train')
        dev = config.get('dev')
        test = config.get('test')
        test2 = config.get('test2')

        load_reranker_weights = config.get('load_reranker_weights')
        save_reranker_weights = config.get('save_reranker_weights')

        # features
        lm = config.get('lm')
        char_nn_lm = config.get('char_nn_lm')
        word_nn_lm = config.get('word_nn_lm')

        # reranker classifier hyperparams
        epochs = config.get('epochs', 5)
        tau = config.get('tau', 500)
        cutoff = config.get('cutoff', 100)
        eta = config.get('eta', 0.05)
        restarts = config.get('restarts', 1)
        compute_exact = config.get('compute_exact')
        sklearn = config.get('sklearn')
        random_seed = config.get('random_seed')
        word_lm_unk_score = config.get('word_lm_unk_score')

        self.output_ranks = config.get('output_ranks', False)
        self.num_threads = config.get('num_threads', 1)

        def path(path: str, project_path: Path) -> Path:
            path = Path(path)
            if not path.exists():
                print(f'File {path} not found. Looking into project path now...')
                path = project_path / path
                if not path.exists():
                    print('Path doesn\'t exist: ', path)
                    path = None
            if path:
                print(f'... file {path} found. OK!')
            return path

        if reranker_dir is not None:
            self.project_path = Path(reranker_dir)
        elif model_dir is not None:
            self.project_path = Path(model_dir)
        else:
            self.project_path = Path('/')

        self.path2dev_json = path(dev_dec_json, self.project_path)
        if test_dec_json:
            self.path2test_json = path(test_dec_json, self.project_path)
        else:
            self.path2test_json = None
        if test2_dec_json:
            self.path2test2_json = path(test2_dec_json, self.project_path)
        else:
            self.path2test2_json = None
        if train is not None:
            path2train_tsv = path(train, self.project_path)
        else:
            path2train_tsv = path('train.tsv', self.project_path)
            if not path2train_tsv.exists():
                path2train_tsv = None
        if dev:
            self.path2dev_tsv = path(dev, self.project_path)
        else:
            self.path2dev_tsv = path('dev.tsv', self.project_path)
        if test:
            self.path2test_tsv = path(test, self.project_path)
        elif self.path2test_json:
            self.path2test_tsv = path('test.tsv', self.project_path)
        else:
            self.path2test_tsv = None
        if test2:
            self.path2test2_tsv = path(test2, self.project_path)
        elif self.path2test2_json:
            self.path2test2_tsv = path('test2.tsv', self.project_path)
        else:
            self.path2test2_tsv = None

        print('Paths:')
        for name, path_obj in (
                ('project_path', self.project_path), ('path2dev_json', self.path2dev_json),
                ('path2test_json', self.path2test_json), ('path2test2_json', self.path2test2_json),
                ('path2train_tsv', path2train_tsv), ('path2dev_tsv', self.path2dev_tsv),
                ('path2test_tsv', self.path2test_tsv), ('path2test2_tsv', self.path2test2_tsv),
                ('lm', lm), ('char_nn_lm', char_nn_lm), ('word_nn_lm', word_nn_lm),
                ('output_ranks', self.output_ranks)):
            print(f'{name: <20} : {path_obj}')
        print()

        if load_reranker_weights:
            path2weights = Path(load_reranker_weights)
            if path2weights.suffix != '.npy':  # fix possibly empty extension
                path2weights = path2weights.parent / (path2weights.stem + '.npy')
            self.path2weights = path(path2weights, self.project_path)
        else:
            self.path2weights = None
        if save_reranker_weights:
            path2save_weights = self.project_path / save_reranker_weights
            if path2save_weights.suffix != '.npy':  # fix possibly empty extension
                self.path2save_weights = path2save_weights.parent / (path2save_weights.stem + '.npy')
            else:
                self.path2save_weights = path2save_weights
            assert not self.path2save_weights.exists(), (
                'Path for saving reranker params exists!', self.path2save_weights)
        else:
            self.path2save_weights = None

        reranker_params = dict(epochs=epochs, tau=tau, cutoff=cutoff, eta=eta, restarts=restarts,
                               upper_bound_exact=800 if compute_exact else 0, num_threads=self.num_threads,
                               random_seed=random_seed)
        print('Reranker training params: ')
        for k, v in reranker_params.items():
            print(f'{k: <40} : {v}')
        if path2train_tsv is not None:
            train_data = list(direct_fn_reader(path2data=path2train_tsv))
        else:
            train_data = []

        if lm:
            language_model = SimpleLanguageModel(path2model=lm)
        else:
            language_model = None

        if char_nn_lm:
            char_nn_language_model = CharLanguageModel.load_language_model(char_nn_lm)
        else:
            char_nn_language_model = None

        if word_nn_lm:
            word_nn_language_model = WordLanguageModel.load_language_model(word_nn_lm)
        else:
            word_nn_language_model = None

        self.score_function = NormalizedEditDistanceMargin()

        self.featurizer = CoreFeaturizer(language_model, train_data, char_nn_language_model, word_nn_language_model,
                                         word_unk_score=word_lm_unk_score)
        if sklearn:
            self.reranker = PROClassifierReranker(SGDClassifier(fit_intercept=False, early_stopping=False,
                                                                max_iter=50, tol=1e-3, n_jobs=-1), **reranker_params)
            print(f'Using the following base classifier: {self.reranker.classifier}')
        else:
            self.reranker = PROReranker(**reranker_params)
            print(f'Using the Cassius Clay base classifier.')

    def rerank(self):

        if self.path2weights:
            self.reranker.load_from_file(self.path2weights)
            print(f'Loaded reranker parameters from {self.path2weights}...')
        else:
            print('** TRAINING **')
            then = time.time()
            print('Featurize data ...')

            predictions = list(dec_json_reader(path2data=self.path2dev_json))
            X, y, truth = self.featurizer.featurize(predictions, self.score_function, num_threads=self.num_threads)
            print(f'Finished featurizing in {(time.time() - then):.3f} sec.')

            then = time.time()
            print('Training ...')
            self.reranker.train(X, y, truth)
            print(f'Finished training in {(time.time() - then):.3f} sec.')

            then = time.time()
            print('Predicting ...')
            reranker_predictions, _ = self.reranker.predict(X, y, truth)
            print(f'Finished predicting in {(time.time() - then):.3f} sec.')

            if self.path2save_weights:
                self.reranker.save_to_file(self.path2save_weights)
                print(f'Saved reranker parameters to {self.path2save_weights}...')

        print('** PREDICTION **')

        for withheld_name, path2tsv, path2json in (('dev', self.path2dev_tsv, self.path2dev_json),
                                                   ('test', self.path2test_tsv, self.path2test_json),
                                                   ('test2', self.path2test2_tsv, self.path2test2_json)):

            print(withheld_name.upper(), '...\n')

            if path2json:

                if withheld_name == 'dev' and len(predictions) > 2000:

                    print('...for speed, reusing featurized dev data.')

                else:
                    predictions = list(dec_json_reader(path2data=path2json))

                    then = time.time()
                    print('Featurize data ...')
                    X, y, truth = self.featurizer.featurize(predictions, self.score_function,
                                                            num_threads=self.num_threads)
                    print(f'Finished featurizing in {(time.time() - then):.3f} sec.')

                if path2tsv:

                    then = time.time()
                    print('Predicting ...')
                    reranker_predictions, _ = self.reranker.predict(X, y, truth)
                    print(f'Finished predicting in {(time.time() - then):.3f} sec.')

                    predictions_fn = self.project_path / f'f.rerank.{withheld_name}.predictions'
                    eval_fn = self.project_path / f'f.rerank.{withheld_name}.eval'
                    # @TODO for tag-stratified eval, launch evalm.py

                    # predictions might have the same source and feats but different targets for dev set!
                    reranker_predict = {(p.source, p.feats): p.hypotheses[r] for p, r in
                                        zip(predictions, reranker_predictions)}
                    correct = 0
                    total = 0
                    lev = []
                    nlev = []
                    with predictions_fn.open(mode='w') as w:
                        for source, target, feats in direct_fn_reader(path2data=path2tsv):
                            predict = reranker_predict[(source, feats)]
                            w.write(f'{source}\t{predict}\t{feats}\n')
                            if target == predict:
                                correct += 1
                            else:
                                ed = editdistance.eval(target, predict)
                                lev.append(ed)
                                nlev.append(ed / len(target))
                            total += 1

                    print(f'Accuracy: {correct * 100 / total:.1f}')
                    with eval_fn.open(mode='w') as w:
                        w.write('MODEL\tTAG\tACCURACY\tLEVDIST\tNORMLEVDIST\n')
                        w.write(f'{predictions_fn}\t---\t{correct / total}\t{np.mean(lev)}\t{np.mean(nlev)}\n')

                if self.output_ranks:
                    then = time.time()
                    print('Outputting ranks and scores for all hypotheses ...')
                    sortings, scores = self.reranker.score(X)
                    print(f'Finished computing ranks in {(time.time() - then):.3f} sec.')

                    rank_dict = dict()
                    score_keys = ['direct_scores', 'lm_scores', 'channel_scores']
                    for p, sorting, scores in zip(predictions, sortings, scores):

                        if p.source in rank_dict:
                            # the scores of its candidates won't be any different, hence:
                            rank_dict[p.source]['target'].append(p.target)
                            rank_dict[p.source]['feats'].append(p.feats)
                        else:
                            reordered_hypotheses = []
                            reordered_actions = []
                            reordered_base_scores = []
                            for i in sorting:
                                reordered_hypotheses.append(p.hypotheses[i])
                                reordered_actions.append(p.actions[i])
                                reordered_base_scores.append(p.scores[i])
                            tmp = dict(
                                target=[p.target],
                                feats=[p.feats],
                                candidates=reordered_hypotheses,
                                actions=reordered_actions,
                                reranker_scores=list(scores[sorting])
                            )
                            reordered_base_scores = zip(*reordered_base_scores)
                            tmp.update(zip(score_keys, reordered_base_scores))
                            rank_dict[p.source]= tmp

                    with (self.project_path / f'f.rerank.{withheld_name}.ranks.json').open(mode='w') as w:
                        json.dump(rank_dict, w, indent=4, ensure_ascii=False)
            else:
                print(f'... skipping as {withheld_name} predictions json is not available.')

        print('Done.')


if __name__ == "__main__":

    import argparse

    parser = argparse.ArgumentParser(description='Train a reranker and decode with it.')
    parser.add_argument('--model-dir', help=('Path to the directory of the trained model. '
                                             'Necessary if the paths to train / dev / test '
                                             '/ decoding / ... files are not specified.'
                                             'Defaults to / if not specified.'),
                        type=str, default=None),
    parser.add_argument('--reranker-dir', help=('Path to the directory of the reranker output.'
                                                'Defaults to model-dir option if not specified.'),
                        type=str, default=None),
    parser.add_argument('--dev-dec-json', help=('Filename of in the `model_dir` or path to the json with hypotheses '
                                                'for the dev set.'), type=str, required=True)
    parser.add_argument('--test-dec-json', help=('Filename of in the `model_dir` or path to the json with hypotheses '
                                                 'for the test set.'), type=str, default=None)
    parser.add_argument('--load-reranker-weights', help='Path to reranker weight file.', type=str, default=None)
    parser.add_argument('--save-reranker-weights', help='Path to file where to save reranker weights.',
                        type=str, default=None)
    parser.add_argument('--lm', help='Path to the kenlm language model.', type=str, required=False)
    parser.add_argument('--char-nn-lm', help='Path to the character-level RNN language model.',
                        type=str, required=False)
    parser.add_argument('--word-nn-lm', help='Path to the word-level RNN language model.', type=str, required=False)
    parser.add_argument('--train', help='Path to the training data.', type=str, default=None)
    parser.add_argument('--dev', help='Path to the dev set data.', type=str, default=None)
    parser.add_argument('--test', help='Path to the test set data.', type=str, default=None)
    parser.add_argument('--epochs', help='Reranker training: Number of training epochs.', type=int, default=5)
    parser.add_argument('--tau', help='Reranker training: Number of samples drawn per input.', type=int, default=500)
    parser.add_argument('--cutoff', help='Reranker training: Number of samples per input to retain.',
                        type=int, default=100)
    parser.add_argument('--eta', help='Reranker training: Perceptron learning rate.', type=float, default=0.05)
    parser.add_argument('--restarts', help='Reranker training: Number of restarts.', type=int, default=1)
    parser.add_argument('--compute-exact', help='Reranker training: Do not sample, compute all 2-combinations.',
                        action='store_true')
    parser.add_argument('--sklearn', help='Reranker training: Use sklearn\'s SVM as base classifier.',
                        action='store_true')
    parser.add_argument('--num-threads', help='Number of threads to use for training.', type=int, default=1)
    parser.add_argument('--random-seed', help='Random seed.', type=int, default=None)
    parser.add_argument('--output-ranks', help=('Additionally, output json with the hypotheses ranked for each '
                                                'scored dataset.'), action='store_true')

    args = parser.parse_args()

    reranker = WordReranker(vars(args))
    reranker.rerank()
